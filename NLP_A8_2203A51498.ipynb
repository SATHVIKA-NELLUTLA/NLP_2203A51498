{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example text data (you can replace this with any larger corpus) text = \"\"\" Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her."
      ],
      "metadata": {
        "id": "54j5438cfQK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Build the Transformer Model on above dataset"
      ],
      "metadata": {
        "id": "FU301TWofX2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R37ajn3DgrYx",
        "outputId": "80463096-5061-4207-d56a-c3dda20143b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchtext\n",
        "!pip install torch==2.0.1 torchtext==0.15.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbeCCrCshDYo",
        "outputId": "4a05bc29-a928-4af4-d626-be9654890237"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1\n",
            "Uninstalling torch-2.0.1:\n",
            "  Successfully uninstalled torch-2.0.1\n",
            "Found existing installation: torchtext 0.15.2\n",
            "Uninstalling torchtext-0.15.2:\n",
            "  Successfully uninstalled torchtext-0.15.2\n",
            "Collecting torch==2.0.1\n",
            "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.2\n",
            "  Using cached torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Using cached torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: torch, torchtext\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1 torchtext-0.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohwGK1gmfO56",
        "outputId": "59031232-521c-4eaf-b2cb-b297c4dbcd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('her', 6), ('a', 4), ('to', 4), ('she', 2), ('grandmother', 2), ('who', 2), ('the', 2), ('woods', 2), ('once', 1), ('upon', 1)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Preprocess text by making lowercase and removing punctuation\n",
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\"\"\"\n",
        "text = text.lower()\n",
        "words = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "# Count word frequency\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Display the 10 most common words\n",
        "print(word_counts.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the pre-trained English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Display named entities\n",
        "for entity in doc.ents:\n",
        "    print(f\"{entity.text} ({entity.label_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcqrX4olfpFQ",
        "outputId": "19e79b61-bcc9-4541-b48a-3f3623658835"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one day (DATE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Get sentiment polarity (-1 to 1, where 1 is very positive)\n",
        "sentiment = blob.sentiment.polarity\n",
        "print(f\"Sentiment polarity: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kv7l415fuvO",
        "outputId": "53e32100-1bb2-40d1-f716-a740557f687f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment polarity: -0.03749999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Sample dataset text\n",
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\"\"\"\n",
        "\n",
        "# Parameters\n",
        "embed_size = 64\n",
        "num_heads = 2\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "seq_length = 10  # sequence length for training\n",
        "batch_size = 2\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "vocab = build_vocab_from_iterator([tokens], specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "# Encode text to integers\n",
        "encoded_text = [vocab[\"<sos>\"]] + [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "# Dataset and DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_length):\n",
        "        self.data = []\n",
        "        for i in range(0, len(encoded_text) - seq_length):\n",
        "            self.data.append((encoded_text[i:i+seq_length], encoded_text[i+1:i+1+seq_length]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "dataset = TextDataset(encoded_text, seq_length)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Step 2: Transformer Model Design\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            embed_size, num_heads, num_layers, num_layers, hidden_size\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "        # Create positional encoding dynamically based on input sequence length and batch size\n",
        "        position = torch.arange(seq_length).unsqueeze(1).expand(seq_length, batch_size).to(x.device)\n",
        "        position = self.embedding(position)  # (seq_length, batch_size, embed_size)\n",
        "\n",
        "        x = self.embedding(x) + position.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        x = x.permute(1, 0, 2)  # (seq_length, batch_size, embed_size) for transformer input\n",
        "        x = self.transformer(x, x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = TransformerModel(len(vocab), embed_size, num_heads, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Step 3: Training the Model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for src, tgt in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src)\n",
        "        loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(loader)}\")\n",
        "\n",
        "# Testing the Model (Generating a sequence)\n",
        "model.eval()\n",
        "start_token = torch.tensor([[vocab[\"<sos>\"]]], dtype=torch.long)\n",
        "generated_text = start_token\n",
        "\n",
        "for _ in range(20):  # Generate 20 tokens\n",
        "    with torch.no_grad():\n",
        "        output = model(generated_text)\n",
        "        next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
        "        generated_text = torch.cat((generated_text, next_token), dim=1)\n",
        "\n",
        "# Convert generated indices back to words\n",
        "generated_words = [vocab.lookup_token(token.item()) for token in generated_text.squeeze()]\n",
        "print(\"Generated text:\", \" \".join(generated_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUaFSuuYgNkN",
        "outputId": "968d2f7e-e812-4364-acfd-01d641da96e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.6525793501308987\n",
            "Epoch 2/10, Loss: 3.073973459856851\n",
            "Epoch 3/10, Loss: 2.378192288534982\n",
            "Epoch 4/10, Loss: 1.7735906158174788\n",
            "Epoch 5/10, Loss: 1.3291738331317902\n",
            "Epoch 6/10, Loss: 0.9994254857301712\n",
            "Epoch 7/10, Loss: 0.7574767470359802\n",
            "Epoch 8/10, Loss: 0.5657120038356099\n",
            "Epoch 9/10, Loss: 0.4475465363689831\n",
            "Epoch 10/10, Loss: 0.3877304904162884\n",
            "Generated text: <sos> wanted wanted to to her on her to her grandmother woods her . her wolf who bad , her named\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) Train the model using 20, 60, 70 epochs"
      ],
      "metadata": {
        "id": "PVoKX8rvkOSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Sample dataset text\n",
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\"\"\"\n",
        "\n",
        "# Parameters\n",
        "embed_size = 64\n",
        "num_heads = 2\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "seq_length = 10  # sequence length for training\n",
        "batch_size = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "vocab = build_vocab_from_iterator([tokens], specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "# Encode text to integers\n",
        "encoded_text = [vocab[\"<sos>\"]] + [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "# Dataset and DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_length):\n",
        "        self.data = []\n",
        "        for i in range(0, len(encoded_text) - seq_length):\n",
        "            self.data.append((encoded_text[i:i+seq_length], encoded_text[i+1:i+1+seq_length]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "dataset = TextDataset(encoded_text, seq_length)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Step 2: Transformer Model Design\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            embed_size, num_heads, num_layers, num_layers, hidden_size\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "        # Create positional encoding dynamically based on input sequence length and batch size\n",
        "        position = torch.arange(seq_length).unsqueeze(1).expand(seq_length, batch_size).to(x.device)\n",
        "        position = self.embedding(position)  # (seq_length, batch_size, embed_size)\n",
        "\n",
        "        x = self.embedding(x) + position.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        x = x.permute(1, 0, 2)  # (seq_length, batch_size, embed_size) for transformer input\n",
        "        x = self.transformer(x, x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, loader, num_epochs):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(loader)}\")\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerModel(len(vocab), embed_size, num_heads, hidden_size, num_layers)\n",
        "\n",
        "# Step 3: Train for 20, 60, and 70 epochs\n",
        "print(\"Training for 20 epochs:\")\n",
        "train_model(model, loader, num_epochs=20)\n",
        "\n",
        "print(\"\\nTraining for 60 epochs:\")\n",
        "train_model(model, loader, num_epochs=60)\n",
        "\n",
        "print(\"\\nTraining for 70 epochs:\")\n",
        "train_model(model, loader, num_epochs=70)\n",
        "\n",
        "# Testing the Model (Generating a sequence)\n",
        "model.eval()\n",
        "start_token = torch.tensor([[vocab[\"<sos>\"]]], dtype=torch.long)\n",
        "generated_text = start_token\n",
        "\n",
        "for _ in range(20):  # Generate 20 tokens\n",
        "    with torch.no_grad():\n",
        "        output = model(generated_text)\n",
        "        next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
        "        generated_text = torch.cat((generated_text, next_token), dim=1)\n",
        "\n",
        "# Convert generated indices back to words\n",
        "generated_words = [vocab.lookup_token(token.item()) for token in generated_text.squeeze()]\n",
        "print(\"Generated text:\", \" \".join(generated_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS5r0MRokUka",
        "outputId": "8bf12a8a-9d7e-4f6b-ef12-8e89e2316982"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 20 epochs:\n",
            "Epoch 1/20, Loss: 3.537820041179657\n",
            "Epoch 2/20, Loss: 2.786639698914119\n",
            "Epoch 3/20, Loss: 2.149489883865629\n",
            "Epoch 4/20, Loss: 1.573677999632699\n",
            "Epoch 5/20, Loss: 1.1476625459534782\n",
            "Epoch 6/20, Loss: 0.8520970003945487\n",
            "Epoch 7/20, Loss: 0.6490336369190898\n",
            "Epoch 8/20, Loss: 0.5016217093382563\n",
            "Epoch 9/20, Loss: 0.4063738211989403\n",
            "Epoch 10/20, Loss: 0.32050611451268196\n",
            "Epoch 11/20, Loss: 0.2712828123143741\n",
            "Epoch 12/20, Loss: 0.23149434556918486\n",
            "Epoch 13/20, Loss: 0.1911866326949426\n",
            "Epoch 14/20, Loss: 0.1644972094467708\n",
            "Epoch 15/20, Loss: 0.15363820189876215\n",
            "Epoch 16/20, Loss: 0.13111823770616735\n",
            "Epoch 17/20, Loss: 0.1150105729965227\n",
            "Epoch 18/20, Loss: 0.09300358859556061\n",
            "Epoch 19/20, Loss: 0.08949809002556972\n",
            "Epoch 20/20, Loss: 0.07557534852198192\n",
            "\n",
            "Training for 60 epochs:\n",
            "Epoch 1/60, Loss: 0.10959914858852114\n",
            "Epoch 2/60, Loss: 0.0807519799896649\n",
            "Epoch 3/60, Loss: 0.08737738942727447\n",
            "Epoch 4/60, Loss: 0.10958111179726464\n",
            "Epoch 5/60, Loss: 0.11494214300598417\n",
            "Epoch 6/60, Loss: 0.1031149048358202\n",
            "Epoch 7/60, Loss: 0.07572101362581764\n",
            "Epoch 8/60, Loss: 0.06807548236767096\n",
            "Epoch 9/60, Loss: 0.0582835898468537\n",
            "Epoch 10/60, Loss: 0.054275565181991885\n",
            "Epoch 11/60, Loss: 0.04597127161520932\n",
            "Epoch 12/60, Loss: 0.04777444240504077\n",
            "Epoch 13/60, Loss: 0.049747543243159144\n",
            "Epoch 14/60, Loss: 0.03960892719416214\n",
            "Epoch 15/60, Loss: 0.034052965214609036\n",
            "Epoch 16/60, Loss: 0.05237679697373616\n",
            "Epoch 17/60, Loss: 0.06322607250019376\n",
            "Epoch 18/60, Loss: 0.06342315206503761\n",
            "Epoch 19/60, Loss: 0.06683612165839545\n",
            "Epoch 20/60, Loss: 0.061915228442688076\n",
            "Epoch 21/60, Loss: 0.07949510469500508\n",
            "Epoch 22/60, Loss: 0.07448649433042322\n",
            "Epoch 23/60, Loss: 0.05951737951753395\n",
            "Epoch 24/60, Loss: 0.04451374732889235\n",
            "Epoch 25/60, Loss: 0.03465350286569446\n",
            "Epoch 26/60, Loss: 0.03666564923644598\n",
            "Epoch 27/60, Loss: 0.04270619815880699\n",
            "Epoch 28/60, Loss: 0.027857389650307596\n",
            "Epoch 29/60, Loss: 0.025551445333154073\n",
            "Epoch 30/60, Loss: 0.036880063283855895\n",
            "Epoch 31/60, Loss: 0.03981071677325027\n",
            "Epoch 32/60, Loss: 0.03414902397863833\n",
            "Epoch 33/60, Loss: 0.03911721066106111\n",
            "Epoch 34/60, Loss: 0.058518939019579976\n",
            "Epoch 35/60, Loss: 0.061380628596193025\n",
            "Epoch 36/60, Loss: 0.026328791182355156\n",
            "Epoch 37/60, Loss: 0.03629180204006843\n",
            "Epoch 38/60, Loss: 0.0497175200975367\n",
            "Epoch 39/60, Loss: 0.05782165567922805\n",
            "Epoch 40/60, Loss: 0.04246305428179247\n",
            "Epoch 41/60, Loss: 0.06304570931908009\n",
            "Epoch 42/60, Loss: 0.043505205789447894\n",
            "Epoch 43/60, Loss: 0.03086191724287346\n",
            "Epoch 44/60, Loss: 0.028237241411781206\n",
            "Epoch 45/60, Loss: 0.019929896033967713\n",
            "Epoch 46/60, Loss: 0.021115829872932017\n",
            "Epoch 47/60, Loss: 0.027517185920649872\n",
            "Epoch 48/60, Loss: 0.014685423795266874\n",
            "Epoch 49/60, Loss: 0.020275845554091836\n",
            "Epoch 50/60, Loss: 0.015347822166014729\n",
            "Epoch 51/60, Loss: 0.03386839552591222\n",
            "Epoch 52/60, Loss: 0.02277236179049526\n",
            "Epoch 53/60, Loss: 0.03693163249408826\n",
            "Epoch 54/60, Loss: 0.04654421567517732\n",
            "Epoch 55/60, Loss: 0.03962759869006861\n",
            "Epoch 56/60, Loss: 0.08455946469413382\n",
            "Epoch 57/60, Loss: 0.09807593633221197\n",
            "Epoch 58/60, Loss: 0.06637957572404828\n",
            "Epoch 59/60, Loss: 0.07007832890043833\n",
            "Epoch 60/60, Loss: 0.06622260135398912\n",
            "\n",
            "Training for 70 epochs:\n",
            "Epoch 1/70, Loss: 0.051321883186964054\n",
            "Epoch 2/70, Loss: 0.08319798551799197\n",
            "Epoch 3/70, Loss: 0.031557129529703944\n",
            "Epoch 4/70, Loss: 0.03847091648328517\n",
            "Epoch 5/70, Loss: 0.020862615621548945\n",
            "Epoch 6/70, Loss: 0.024549062800360844\n",
            "Epoch 7/70, Loss: 0.014244925085222349\n",
            "Epoch 8/70, Loss: 0.018416169585959454\n",
            "Epoch 9/70, Loss: 0.017160607907239216\n",
            "Epoch 10/70, Loss: 0.03887556064624472\n",
            "Epoch 11/70, Loss: 0.043340569166632904\n",
            "Epoch 12/70, Loss: 0.03136010539102634\n",
            "Epoch 13/70, Loss: 0.023116775308153592\n",
            "Epoch 14/70, Loss: 0.016728231184450642\n",
            "Epoch 15/70, Loss: 0.020386788233216584\n",
            "Epoch 16/70, Loss: 0.020534886768603298\n",
            "Epoch 17/70, Loss: 0.016456019831821322\n",
            "Epoch 18/70, Loss: 0.022323153958755677\n",
            "Epoch 19/70, Loss: 0.036040337585810836\n",
            "Epoch 20/70, Loss: 0.05377198280101376\n",
            "Epoch 21/70, Loss: 0.046288268050245406\n",
            "Epoch 22/70, Loss: 0.03960800520144403\n",
            "Epoch 23/70, Loss: 0.031923240079777315\n",
            "Epoch 24/70, Loss: 0.06192439714296987\n",
            "Epoch 25/70, Loss: 0.0646839950433267\n",
            "Epoch 26/70, Loss: 0.06136542553680816\n",
            "Epoch 27/70, Loss: 0.07123261972862695\n",
            "Epoch 28/70, Loss: 0.08527133632950219\n",
            "Epoch 29/70, Loss: 0.06485358051889177\n",
            "Epoch 30/70, Loss: 0.04870729814034088\n",
            "Epoch 31/70, Loss: 0.027053181314840913\n",
            "Epoch 32/70, Loss: 0.027969558702482442\n",
            "Epoch 33/70, Loss: 0.053649346310911436\n",
            "Epoch 34/70, Loss: 0.044772718650554974\n",
            "Epoch 35/70, Loss: 0.05298906882983699\n",
            "Epoch 36/70, Loss: 0.043869488783197345\n",
            "Epoch 37/70, Loss: 0.03861302546699465\n",
            "Epoch 38/70, Loss: 0.025099682447034866\n",
            "Epoch 39/70, Loss: 0.019577997672188627\n",
            "Epoch 40/70, Loss: 0.019059280787977122\n",
            "Epoch 41/70, Loss: 0.027758261829148978\n",
            "Epoch 42/70, Loss: 0.022067371219496375\n",
            "Epoch 43/70, Loss: 0.016810432226131006\n",
            "Epoch 44/70, Loss: 0.012605554270391752\n",
            "Epoch 45/70, Loss: 0.017634227585014223\n",
            "Epoch 46/70, Loss: 0.015597045043250546\n",
            "Epoch 47/70, Loss: 0.017509561946748624\n",
            "Epoch 48/70, Loss: 0.017532243285261626\n",
            "Epoch 49/70, Loss: 0.010638827297952957\n",
            "Epoch 50/70, Loss: 0.020246510143416736\n",
            "Epoch 51/70, Loss: 0.01916672253719298\n",
            "Epoch 52/70, Loss: 0.01622179188208455\n",
            "Epoch 53/70, Loss: 0.02195082899977154\n",
            "Epoch 54/70, Loss: 0.014678822565786374\n",
            "Epoch 55/70, Loss: 0.013751981828785833\n",
            "Epoch 56/70, Loss: 0.02486799728857087\n",
            "Epoch 57/70, Loss: 0.015716251431773083\n",
            "Epoch 58/70, Loss: 0.016788962430187633\n",
            "Epoch 59/70, Loss: 0.01495997010663684\n",
            "Epoch 60/70, Loss: 0.008897372936709351\n",
            "Epoch 61/70, Loss: 0.040088215752413295\n",
            "Epoch 62/70, Loss: 0.02848040968404218\n",
            "Epoch 63/70, Loss: 0.03243428765979063\n",
            "Epoch 64/70, Loss: 0.01849738297044366\n",
            "Epoch 65/70, Loss: 0.05079174041332278\n",
            "Epoch 66/70, Loss: 0.05086246677508045\n",
            "Epoch 67/70, Loss: 0.07371761333862585\n",
            "Epoch 68/70, Loss: 0.16253128970441008\n",
            "Epoch 69/70, Loss: 0.10607731879489231\n",
            "Epoch 70/70, Loss: 0.11682413285598159\n",
            "Generated text: <sos> in the woods . one day , her mother asked woods lived . one day of bad , her named\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iii) After training, use the model to generate new text by feeding it an initial seed text"
      ],
      "metadata": {
        "id": "nEYcs7YNkzua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Sample dataset text\n",
        "text = \"\"\"Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods. One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\"\"\"\n",
        "\n",
        "# Parameters\n",
        "embed_size = 64\n",
        "num_heads = 2\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "seq_length = 10  # sequence length for training\n",
        "batch_size = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "vocab = build_vocab_from_iterator([tokens], specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "# Encode text to integers\n",
        "encoded_text = [vocab[\"<sos>\"]] + [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "# Dataset and DataLoader\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_length):\n",
        "        self.data = []\n",
        "        for i in range(0, len(encoded_text) - seq_length):\n",
        "            self.data.append((encoded_text[i:i+seq_length], encoded_text[i+1:i+1+seq_length]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "dataset = TextDataset(encoded_text, seq_length)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Step 2: Transformer Model Design\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            embed_size, num_heads, num_layers, num_layers, hidden_size\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "        position = torch.arange(seq_length).unsqueeze(1).expand(seq_length, batch_size).to(x.device)\n",
        "        position = self.embedding(position)  # (seq_length, batch_size, embed_size)\n",
        "\n",
        "        x = self.embedding(x) + position.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        x = x.permute(1, 0, 2)  # (seq_length, batch_size, embed_size) for transformer input\n",
        "        x = self.transformer(x, x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, loader, num_epochs):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(loader)}\")\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerModel(len(vocab), embed_size, num_heads, hidden_size, num_layers)\n",
        "\n",
        "# Step 3: Train for 20, 60, and 70 epochs\n",
        "print(\"Training for 20 epochs:\")\n",
        "train_model(model, loader, num_epochs=20)\n",
        "\n",
        "print(\"\\nTraining for 60 epochs:\")\n",
        "train_model(model, loader, num_epochs=60)\n",
        "\n",
        "print(\"\\nTraining for 70 epochs:\")\n",
        "train_model(model, loader, num_epochs=70)\n",
        "\n",
        "# Function to generate text given a seed\n",
        "def generate_text(model, seed_text, max_length=20):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode the seed text\n",
        "    seed_tokens = tokenizer(seed_text)\n",
        "    seed_indices = [vocab[\"<sos>\"]] + [vocab[token] for token in seed_tokens]\n",
        "    input_tensor = torch.tensor(seed_indices).unsqueeze(0)  # (1, seq_length + 1)\n",
        "\n",
        "    generated_text = input_tensor\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output = model(generated_text)\n",
        "            next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)  # Get last token prediction\n",
        "            generated_text = torch.cat((generated_text, next_token), dim=1)  # Append predicted token\n",
        "\n",
        "    # Convert generated indices back to words\n",
        "    generated_words = [vocab.lookup_token(token.item()) for token in generated_text.squeeze()]\n",
        "    return \" \".join(generated_words)\n",
        "\n",
        "# Example usage of the generate_text function\n",
        "seed_input = \"Once upon a time\"\n",
        "generated_output = generate_text(model, seed_input)\n",
        "print(\"\\nGenerated text from seed input:\")\n",
        "print(generated_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyiDjmmpk3A3",
        "outputId": "0223fbb0-a277-49c7-f96d-1d3e8015d398"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 20 epochs:\n",
            "Epoch 1/20, Loss: 3.6034443037850514\n",
            "Epoch 2/20, Loss: 2.968920741762434\n",
            "Epoch 3/20, Loss: 2.2996141484805515\n",
            "Epoch 4/20, Loss: 1.713986839566912\n",
            "Epoch 5/20, Loss: 1.2770515148128783\n",
            "Epoch 6/20, Loss: 0.9161859729460308\n",
            "Epoch 7/20, Loss: 0.7003728513206754\n",
            "Epoch 8/20, Loss: 0.5369263525520053\n",
            "Epoch 9/20, Loss: 0.43426841497421265\n",
            "Epoch 10/20, Loss: 0.3448526268558843\n",
            "Epoch 11/20, Loss: 0.283246819462095\n",
            "Epoch 12/20, Loss: 0.2381653248199395\n",
            "Epoch 13/20, Loss: 0.202905303931662\n",
            "Epoch 14/20, Loss: 0.18809998274913856\n",
            "Epoch 15/20, Loss: 0.18738360357071673\n",
            "Epoch 16/20, Loss: 0.16752104966768197\n",
            "Epoch 17/20, Loss: 0.14597733997340714\n",
            "Epoch 18/20, Loss: 0.10569844833974328\n",
            "Epoch 19/20, Loss: 0.09927455921258245\n",
            "Epoch 20/20, Loss: 0.07589996632720743\n",
            "\n",
            "Training for 60 epochs:\n",
            "Epoch 1/60, Loss: 0.10270796649690185\n",
            "Epoch 2/60, Loss: 0.10325740404160959\n",
            "Epoch 3/60, Loss: 0.11446882079222373\n",
            "Epoch 4/60, Loss: 0.10268178582191467\n",
            "Epoch 5/60, Loss: 0.07940549217164516\n",
            "Epoch 6/60, Loss: 0.06117991946770677\n",
            "Epoch 7/60, Loss: 0.04341071089064436\n",
            "Epoch 8/60, Loss: 0.05706518868516598\n",
            "Epoch 9/60, Loss: 0.05609587623205568\n",
            "Epoch 10/60, Loss: 0.11201299903249103\n",
            "Epoch 11/60, Loss: 0.07118567185742515\n",
            "Epoch 12/60, Loss: 0.048775464396125505\n",
            "Epoch 13/60, Loss: 0.08305707617130663\n",
            "Epoch 14/60, Loss: 0.058940100649903925\n",
            "Epoch 15/60, Loss: 0.05915843662140625\n",
            "Epoch 16/60, Loss: 0.037631297267840376\n",
            "Epoch 17/60, Loss: 0.049051069089078476\n",
            "Epoch 18/60, Loss: 0.06706599299130696\n",
            "Epoch 19/60, Loss: 0.07603792862833611\n",
            "Epoch 20/60, Loss: 0.07662775066481638\n",
            "Epoch 21/60, Loss: 0.05785484831514103\n",
            "Epoch 22/60, Loss: 0.06427036284003407\n",
            "Epoch 23/60, Loss: 0.07324447494465858\n",
            "Epoch 24/60, Loss: 0.05702784028835595\n",
            "Epoch 25/60, Loss: 0.055855483116049855\n",
            "Epoch 26/60, Loss: 0.037708905625290105\n",
            "Epoch 27/60, Loss: 0.03948371218783515\n",
            "Epoch 28/60, Loss: 0.06787560632385846\n",
            "Epoch 29/60, Loss: 0.06642680778168142\n",
            "Epoch 30/60, Loss: 0.04580168253076928\n",
            "Epoch 31/60, Loss: 0.03758566412476024\n",
            "Epoch 32/60, Loss: 0.04127322430057185\n",
            "Epoch 33/60, Loss: 0.049629241053480655\n",
            "Epoch 34/60, Loss: 0.04840160984479423\n",
            "Epoch 35/60, Loss: 0.02908547660003283\n",
            "Epoch 36/60, Loss: 0.023191452504501546\n",
            "Epoch 37/60, Loss: 0.030430941326942827\n",
            "Epoch 38/60, Loss: 0.03569504086044617\n",
            "Epoch 39/60, Loss: 0.04134364723826626\n",
            "Epoch 40/60, Loss: 0.057233170340103764\n",
            "Epoch 41/60, Loss: 0.04637054430453905\n",
            "Epoch 42/60, Loss: 0.05407960118360019\n",
            "Epoch 43/60, Loss: 0.02387852651632524\n",
            "Epoch 44/60, Loss: 0.02496990774359022\n",
            "Epoch 45/60, Loss: 0.04968165568841089\n",
            "Epoch 46/60, Loss: 0.0457172553287819\n",
            "Epoch 47/60, Loss: 0.041242214501835406\n",
            "Epoch 48/60, Loss: 0.049374448684310276\n",
            "Epoch 49/60, Loss: 0.03852467049312379\n",
            "Epoch 50/60, Loss: 0.03737569343398458\n",
            "Epoch 51/60, Loss: 0.06360538805269503\n",
            "Epoch 52/60, Loss: 0.042256698195290356\n",
            "Epoch 53/60, Loss: 0.03587677633290046\n",
            "Epoch 54/60, Loss: 0.04402967674624441\n",
            "Epoch 55/60, Loss: 0.03034293753444217\n",
            "Epoch 56/60, Loss: 0.02519610352257067\n",
            "Epoch 57/60, Loss: 0.030762635990478366\n",
            "Epoch 58/60, Loss: 0.022684109571855515\n",
            "Epoch 59/60, Loss: 0.01617197728150391\n",
            "Epoch 60/60, Loss: 0.01855022367505756\n",
            "\n",
            "Training for 70 epochs:\n",
            "Epoch 1/70, Loss: 0.023677370835295215\n",
            "Epoch 2/70, Loss: 0.06699138906385217\n",
            "Epoch 3/70, Loss: 0.06820395961819616\n",
            "Epoch 4/70, Loss: 0.03848301364009136\n",
            "Epoch 5/70, Loss: 0.03296765643504581\n",
            "Epoch 6/70, Loss: 0.03127777623428431\n",
            "Epoch 7/70, Loss: 0.017526487728381262\n",
            "Epoch 8/70, Loss: 0.01838316159306227\n",
            "Epoch 9/70, Loss: 0.04623996693616001\n",
            "Epoch 10/70, Loss: 0.05996818664633403\n",
            "Epoch 11/70, Loss: 0.04445463717482718\n",
            "Epoch 12/70, Loss: 0.02247599627922422\n",
            "Epoch 13/70, Loss: 0.02101366929543604\n",
            "Epoch 14/70, Loss: 0.02356520529636847\n",
            "Epoch 15/70, Loss: 0.019250135199399665\n",
            "Epoch 16/70, Loss: 0.009733319902027558\n",
            "Epoch 17/70, Loss: 0.01169831417376242\n",
            "Epoch 18/70, Loss: 0.014993964790067236\n",
            "Epoch 19/70, Loss: 0.013422440670962845\n",
            "Epoch 20/70, Loss: 0.013832492166589614\n",
            "Epoch 21/70, Loss: 0.010568551523777257\n",
            "Epoch 22/70, Loss: 0.015248968405233296\n",
            "Epoch 23/70, Loss: 0.01504009666262261\n",
            "Epoch 24/70, Loss: 0.012368266035731981\n",
            "Epoch 25/70, Loss: 0.010909184724109114\n",
            "Epoch 26/70, Loss: 0.042398361887275575\n",
            "Epoch 27/70, Loss: 0.01698213411769497\n",
            "Epoch 28/70, Loss: 0.01636593313638254\n",
            "Epoch 29/70, Loss: 0.01982750726269192\n",
            "Epoch 30/70, Loss: 0.09002174149749667\n",
            "Epoch 31/70, Loss: 0.11455930327896827\n",
            "Epoch 32/70, Loss: 0.10830841572689158\n",
            "Epoch 33/70, Loss: 0.15354509200551547\n",
            "Epoch 34/70, Loss: 0.09337628346734814\n",
            "Epoch 35/70, Loss: 0.10001926767706339\n",
            "Epoch 36/70, Loss: 0.06437295003394995\n",
            "Epoch 37/70, Loss: 0.050384016000732244\n",
            "Epoch 38/70, Loss: 0.06512878025699008\n",
            "Epoch 39/70, Loss: 0.05112681724131107\n",
            "Epoch 40/70, Loss: 0.031216306996481893\n",
            "Epoch 41/70, Loss: 0.03950400494795758\n",
            "Epoch 42/70, Loss: 0.035184906883971835\n",
            "Epoch 43/70, Loss: 0.027907964262080247\n",
            "Epoch 44/70, Loss: 0.03019135360331607\n",
            "Epoch 45/70, Loss: 0.023449439447306628\n",
            "Epoch 46/70, Loss: 0.017239552361258705\n",
            "Epoch 47/70, Loss: 0.017387308510868543\n",
            "Epoch 48/70, Loss: 0.013607205254499734\n",
            "Epoch 49/70, Loss: 0.016966331827071763\n",
            "Epoch 50/70, Loss: 0.02018986772912155\n",
            "Epoch 51/70, Loss: 0.011385868172510527\n",
            "Epoch 52/70, Loss: 0.015716900849448785\n",
            "Epoch 53/70, Loss: 0.017608880270147762\n",
            "Epoch 54/70, Loss: 0.01531250819867377\n",
            "Epoch 55/70, Loss: 0.011148633863610615\n",
            "Epoch 56/70, Loss: 0.011476946140256976\n",
            "Epoch 57/70, Loss: 0.015189255873597826\n",
            "Epoch 58/70, Loss: 0.01698797993386896\n",
            "Epoch 59/70, Loss: 0.020236871307133697\n",
            "Epoch 60/70, Loss: 0.014948874506184697\n",
            "Epoch 61/70, Loss: 0.016759707672463264\n",
            "Epoch 62/70, Loss: 0.04230963734568961\n",
            "Epoch 63/70, Loss: 0.034582084352483174\n",
            "Epoch 64/70, Loss: 0.06008390798732372\n",
            "Epoch 65/70, Loss: 0.041158434088824185\n",
            "Epoch 66/70, Loss: 0.04383626052211704\n",
            "Epoch 67/70, Loss: 0.0506494811686155\n",
            "Epoch 68/70, Loss: 0.07372877084084653\n",
            "Epoch 69/70, Loss: 0.045999230303485614\n",
            "Epoch 70/70, Loss: 0.03738914614742888\n",
            "\n",
            "Generated text from seed input:\n",
            "<sos> once upon a time , there was a little girl woods . , her wolf of goodies , her to to . the girl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iv) Experimenting and Improving the Model by large dataset and hyper tune parameter."
      ],
      "metadata": {
        "id": "Edow6dtflsHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Sample dataset as a string\n",
        "sample_data = \"\"\"Once upon a time, in a faraway land, there lived a young girl named Red Riding Hood.\n",
        "She often visited her grandmother who lived deep in the woods.\n",
        "One day, her mother asked her to take a basket of goodies to her grandmother.\n",
        "As she walked through the forest, she encountered a big bad wolf.\n",
        "The wolf had a cunning plan to trick her, but Red Riding Hood was clever.\n",
        "She eventually reached her grandmother's house, where a surprise awaited her.\"\"\"\n",
        "\n",
        "# Load the sample data into a text variable\n",
        "text = sample_data\n",
        "\n",
        "# Parameters for experimentation\n",
        "embed_size = 128  # Experiment with different sizes\n",
        "num_heads = 4     # Experiment with different number of heads\n",
        "hidden_size = 256 # Adjust hidden size\n",
        "num_layers = 4    # Increase layers for deeper models\n",
        "seq_length = 20   # Increase sequence length if necessary\n",
        "batch_size = 16   # Experiment with different batch sizes\n",
        "learning_rate = 0.0005  # Adjust learning rate\n",
        "num_epochs = 20  # Set the number of epochs for training\n",
        "\n",
        "# Data Preparation\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(text)\n",
        "vocab = build_vocab_from_iterator([tokens], specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "\n",
        "# Encode text to integers\n",
        "encoded_text = [vocab[\"<sos>\"]] + [vocab[token] for token in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "# Custom Dataset Class for Larger Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encoded_text, seq_length):\n",
        "        self.data = []\n",
        "        for i in range(0, len(encoded_text) - seq_length):\n",
        "            self.data.append((encoded_text[i:i + seq_length], encoded_text[i + 1:i + 1 + seq_length]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
        "\n",
        "dataset = TextDataset(encoded_text, seq_length)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_size, num_layers):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer = nn.Transformer(embed_size, num_heads, num_layers, num_layers, hidden_size)\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "        position = torch.arange(seq_length).unsqueeze(1).expand(seq_length, batch_size).to(x.device)\n",
        "        position = self.embedding(position)  # (seq_length, batch_size, embed_size)\n",
        "\n",
        "        x = self.embedding(x) + position.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        x = x.permute(1, 0, 2)  # (seq_length, batch_size, embed_size)\n",
        "        x = self.transformer(x, x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_length, embed_size)\n",
        "        out = self.fc_out(x)\n",
        "        return out\n",
        "\n",
        "# Function to train the model with early stopping\n",
        "def train_model(model, loader, num_epochs, patience=5):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src)\n",
        "            loss = criterion(output.view(-1, len(vocab)), tgt.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerModel(len(vocab), embed_size, num_heads, hidden_size, num_layers)\n",
        "\n",
        "# Train model\n",
        "train_model(model, loader, num_epochs)\n",
        "\n",
        "# Function to generate text given a seed\n",
        "def generate_text(model, seed_text, max_length=20):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode the seed text\n",
        "    seed_tokens = tokenizer(seed_text)\n",
        "    seed_indices = [vocab[\"<sos>\"]] + [vocab[token] for token in seed_tokens]\n",
        "    input_tensor = torch.tensor(seed_indices).unsqueeze(0)  # (1, seq_length + 1)\n",
        "\n",
        "    generated_text = input_tensor\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output = model(generated_text)\n",
        "            next_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)  # Get last token prediction\n",
        "            generated_text = torch.cat((generated_text, next_token), dim=1)  # Append predicted token\n",
        "\n",
        "    # Convert generated indices back to words\n",
        "    generated_words = [vocab.lookup_token(token.item()) for token in generated_text.squeeze()]\n",
        "    return \" \".join(generated_words)\n",
        "\n",
        "# Example usage of the generate_text function\n",
        "seed_input = \"Once upon a time\"\n",
        "generated_output = generate_text(model, seed_input)\n",
        "print(\"\\nGenerated text from seed input:\")\n",
        "print(generated_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohnBPzcDnlt8",
        "outputId": "b5d31548-1974-4588-b16a-7177573d28a6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 4.0168\n",
            "Epoch 2/20, Loss: 3.7676\n",
            "Epoch 3/20, Loss: 3.5672\n",
            "Epoch 4/20, Loss: 3.3141\n",
            "Epoch 5/20, Loss: 3.0366\n",
            "Epoch 6/20, Loss: 2.7172\n",
            "Epoch 7/20, Loss: 2.4321\n",
            "Epoch 8/20, Loss: 2.1609\n",
            "Epoch 9/20, Loss: 1.9021\n",
            "Epoch 10/20, Loss: 1.6596\n",
            "Epoch 11/20, Loss: 1.4840\n",
            "Epoch 12/20, Loss: 1.3177\n",
            "Epoch 13/20, Loss: 1.1662\n",
            "Epoch 14/20, Loss: 1.0351\n",
            "Epoch 15/20, Loss: 0.9366\n",
            "Epoch 16/20, Loss: 0.8411\n",
            "Epoch 17/20, Loss: 0.7627\n",
            "Epoch 18/20, Loss: 0.6832\n",
            "Epoch 19/20, Loss: 0.6418\n",
            "Epoch 20/20, Loss: 0.5861\n",
            "\n",
            "Generated text from seed input:\n",
            "<sos> once upon a time , a faraway land , she encountered a faraway land , a faraway land , there lived wolf of bad\n"
          ]
        }
      ]
    }
  ]
}